\cleardoublepage
\chapter{Implementation}
\label{ch:implementation}
Chapter 4 describes the implementation of the thesis.
This includes how the thesis group chose to implement the design discussed in previous chapters, and the research methods used to arrive at design decisions.

\section{REST API}\label{sec:REST API}

The API exposes several endpoints that allows for saving an image and a receipt to a database.
The API is also connected to the ML and OCR module.

\section{Web solution}\label{sec:Web solution}

The web solution is made with React in JavaScript.
The purpose of the web solution is to give the user an interface where they can upload an image of a receipt.
And get the corresponding data from the image, and then send in the receipt with the data from the image.
This is to make the process of entering the data less time consuming.

When the user uploads an image, the web solution sends a POST request to the API, then the API saves the image to
the database.
When the image is saved, the image is sent to the OCR for text recognition.
If the output makes sense the data is returned and displayed to the user.
Then if the returned data is correct the user can then upload the receipt to the database.

\section{Dataset}\label{dataset}

The dataset provided by Simployer consists of 1194 unlabeled images and pdfs.
However, the variance of the types of receipts in the dataset is very large.
Because of this, we end up with a very small amount of images of each type after labeling them.
The dataset also includes receipts in several languages, mainly norwegian, english and polish.
We filtered the dataset to only include norwegian receipts and removing images of bad quality.
At the end of this process, the amount of usable data was very small.
The original dataset of 1194 images has been filtered down to 91 images, split into three different types of receipts.

\section{Pre-processing}\label{pre-processing}

\subsection{Data augmentation}\label{data augmentation}
Because of the low amount of images in the filtered dataset, data augmentation has to be done in order to give us enough images to be able to effectively train the CNN.
We used an open source data augmentation tool for images for this task LINK.
The augmentor takes a directory of images and applies a series of transformations on the images like scaling, skewing and rotating.
This creates copies of the images with slight differences, increasing the size of our dataset.
After applying enough transformations, the size of the dataset grew from 91 images to 1500.

\subsection{Image preparation}\label{imgprep}
Many of the images in the dataset are taken in different resolutions.
In order to prepare the images to be passed forward to the CNN model, the images resolution is rescaled to 60x60 pixels.
This significantly reduces the training time required by the network with minimal to no loss in accuracy.
The images are also converted to grayscale instead of RGB.
This reduces the amount input nodes required by the network, as each pixel will have one value instead of three.

\section{Convolutional Neural Network}\label{sec:cnn}

Our CNN is implemented using Keras.
Keras is a library for tensorflow that allows for the creation of neural network models with only a few lines of code.
The model consist of an input layer, two hidden layers, and an output layer.
The input layer and first hidden layer are convolutional layers with 64 nodes in each layer.
The second hidden layer and the output layer are dense layers, where the hidden layer has 64 nodes and the output layer has 3 nodes.
The activation function of the first three layers is Rectified Linear Unit (ReLU), while the output layer uses the sigmoid function.
The model uses the Adam optimizer and the categorical crossentropy loss function.







